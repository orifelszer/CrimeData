{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNF5SGafA0RhjmItOL5Vacm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orifelszer/CrimeData/blob/eden-branch/Creating_file.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "iY23eswYQBHB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "7d922bf1-d65d-4467-fa56-5eba79c18055"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'Preprocessing_Multitask_Updated' from 'Data_Handling' (/content/Data_Handling.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-fe65833fe7d2>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mData_Handling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocessing_Multitask_Updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Preprocessing_Multitask_Updated' from 'Data_Handling' (/content/Data_Handling.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvCO0nXpORID",
        "outputId": "3b823d9a-1797-4de0-d573-26cd237f2dc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-06 12:31:15--  https://raw.githubusercontent.com/orifelszer/CrimeData/main/Data_Handling.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5256 (5.1K) [text/plain]\n",
            "Saving to: ‘Data_Handling.py’\n",
            "\n",
            "\rData_Handling.py      0%[                    ]       0  --.-KB/s               \rData_Handling.py    100%[===================>]   5.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-01-06 12:31:15 (38.3 MB/s) - ‘Data_Handling.py’ saved [5256/5256]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# הורדת הקובץ מ-GitHub\n",
        "!wget -O Data_Handling.py \"https://raw.githubusercontent.com/orifelszer/CrimeData/main/Data_Handling.py\"\n",
        "\n",
        "# ייבוא הפונקציה מהקובץ\n",
        "from Data_Handling import Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Paths to Crime Datasets (2019–2024)"
      ],
      "metadata": {
        "id": "HH7ESwFcOeWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/orifelszer/CrimeData.git\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "zip_folder = 'CrimeData'\n",
        "zip_files = [f for f in os.listdir(zip_folder) if f.endswith('.zip')]\n",
        "\n",
        "for zip_file in zip_files:\n",
        "    zip_path = os.path.join(zip_folder, zip_file)\n",
        "    extract_path = os.path.join(zip_folder, zip_file.replace('.zip', ''))\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(f\"Extracted: {zip_file} -> {extract_path}\")"
      ],
      "metadata": {
        "id": "jNxHCYGhOb6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Crime Data into Pandas DataFrames"
      ],
      "metadata": {
        "id": "FdI-ivysOl8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_files = glob.glob(\"CrimeData/**/*.csv\", recursive=True)\n",
        "dataframes = {f\"Crimes_{file[-8:-4]}\": pd.read_csv(file) for file in csv_files}"
      ],
      "metadata": {
        "id": "sLByLu1aOi9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data = pd.concat(dataframes, axis=0, ignore_index=True)"
      ],
      "metadata": {
        "id": "1QlD8nqQOqfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# הסרת שורות עם ערך -1 בעמודת StatisticGroupKod\n",
        "combined_data = combined_data[combined_data['StatisticGroupKod'] != -1]"
      ],
      "metadata": {
        "id": "cItUkZWDOtLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_city = combined_data['Yeshuv']  # מיקום הפשע\n",
        "y_crime_type = combined_data['StatisticGroup']  # סוג הפשע"
      ],
      "metadata": {
        "id": "xHySTB66hDeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# הסרת עמודות החיזוי מהנתונים\n",
        "features = combined_data.drop(['StatisticGroup', 'StatisticGroupKod', 'Yeshuv', 'YeshuvKod'], axis=1)\n",
        "\n",
        "# ✅ חלוקה לסטים: המאפיינים מופרדים מעמודות היעד\n",
        "X_train, X_test, y_city_train, y_city_test, y_crime_train, y_crime_test = train_test_split(\n",
        "    features, y_city, y_crime_type, test_size=0.2, random_state=42, stratify=y_crime_type)"
      ],
      "metadata": {
        "id": "I4UsQaPfPAh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ שימוש בפונקציה המעודכנת ללמידה רב-משתנית\n",
        "X_train_cleaned, _, _, train_mappings, scaler = Preprocessing_Multitask_Updated(X_train, fit_scaler=True)\n",
        "X_test_cleaned, _, _, _, _ = Preprocessing_Multitask_Updated(X_test, train_mappings=train_mappings, scaler=scaler)"
      ],
      "metadata": {
        "id": "TQBq583ZPDMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# הגדרת עמודות לשמירה על בסיס סט האימון בלבד\n",
        "columns_to_keep = [col for col in X_train_cleaned.columns if X_train_cleaned[col].sum() >= 10]\n",
        "\n",
        "# שמירה על עמודות אימון רלוונטיות בלבד\n",
        "X_train_cleaned = X_train_cleaned[columns_to_keep]\n",
        "\n",
        "# התאמת סט המבחן רק לפי מה שנקבע באימון\n",
        "for col in columns_to_keep:\n",
        "    if col not in X_test_cleaned.columns:\n",
        "        X_test_cleaned[col] = 0\n",
        "\n",
        "# הבטחה ששני הסטים באותו סדר עמודות\n",
        "X_test_cleaned = X_test_cleaned[X_train_cleaned.columns]"
      ],
      "metadata": {
        "id": "kMIYcWlpPGNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_city_train = y_city_train.loc[X_train_cleaned.index]\n",
        "y_city_test = y_city_test.loc[X_test_cleaned.index]\n",
        "y_crime_train = y_crime_train.loc[X_train_cleaned.index]\n",
        "y_crime_test = y_crime_test.loc[X_test_cleaned.index]"
      ],
      "metadata": {
        "id": "tEuKxKWSPJzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reducing memory usage\n",
        "def optimize_data_types(df):\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        # אם מדובר במשתנה קטגוריאלי טקסטואלי\n",
        "        if col_type == 'object':\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "        # אם מדובר במשתנה מספרי רציף\n",
        "        elif col_type == 'float64':\n",
        "            df[col] = df[col].astype('float32')\n",
        "\n",
        "        # אם מדובר במשתנה מספרי שלם\n",
        "        elif col_type == 'int64':\n",
        "            df[col] = df[col].astype('int32')\n",
        "    return df\n",
        "\n",
        "# החלת הפונקציה על סט הנתונים\n",
        "X_train_cleaned = optimize_data_types(X_train_cleaned)\n",
        "X_test_cleaned = optimize_data_types(X_test_cleaned)"
      ],
      "metadata": {
        "id": "VJyYCNIHPLtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# קידוד One-Hot עבור עמודות היעד\n",
        "y_city_train_encoded = pd.get_dummies(y_city_train, prefix=\"city\")\n",
        "y_city_test_encoded = pd.get_dummies(y_city_test, prefix=\"city\")\n",
        "\n",
        "y_crime_train_encoded = pd.get_dummies(y_crime_train, prefix=\"crime\")\n",
        "y_crime_test_encoded = pd.get_dummies(y_crime_test, prefix=\"crime\")"
      ],
      "metadata": {
        "id": "Qrdfjkscl0ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# שמירת היעדים המקודדים כקבצי CSV\n",
        "X_train_cleaned.to_csv('X_train.csv', index=False)\n",
        "X_test_cleaned.to_csv('X_test.csv', index=False)\n",
        "y_city_train_encoded.to_csv('y_city_train_encoded.csv', index=False)\n",
        "y_city_test_encoded.to_csv('y_city_test_encoded.csv', index=False)\n",
        "y_crime_train_encoded.to_csv('y_crime_train_encoded.csv', index=False)\n",
        "y_crime_test_encoded.to_csv('y_crime_test_encoded.csv', index=False)"
      ],
      "metadata": {
        "id": "u2WiIdnETJFd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}