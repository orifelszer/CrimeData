{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnJz7rDy0RrYwr+Y7e62ZF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orifelszer/CrimeData/blob/eden-branch/Prepare_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries for data preprocessing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "def Preprocessing_Multitask_Updated(datasets, train_mappings=None, scaler=None, fit_scaler=False):\n",
        "    #We will remove the columns \"municipalKod\" and \"municipalName\" because more than 85% of the data are missing values.\n",
        "    columns_to_remove_85 = ['municipalKod', 'municipalName']\n",
        "    datasets = datasets.drop(columns=columns_to_remove_85, errors='ignore')\n",
        "\n",
        "    #We will remove StatisticTypeKod and 'StatisticType' because our target column is 'StatisticGroup\" and 'StatisticType' belongs to 'StatisticGroup'.\n",
        "    datasets = datasets.drop(columns=['StatisticTypeKod', 'StatisticType'], errors='ignore')\n",
        "    #We will remove 'StatisticAreaKod' and 'StatisticArea' because our target column is 'Yeshuv\" and 'StatisticAreaKod' belongs to 'Yeshuv'.\n",
        "    datasets = datasets.drop(columns=['StatisticAreaKod', 'StatisticArea'], errors='ignore')\n",
        "\n",
        "    #We removed the columns \"PoliceMerhavKod\", \"PoliceDistrictKod\", and \"PoliceStationKod\" because they are either redundant, irrelevant to the analysis.\n",
        "    columns_to_remove_redundant = ['PoliceMerhavKod', 'PoliceDistrictKod', 'PoliceStationKod', 'PoliceMerhav', 'PoliceDistrict', 'StatisticArea']\n",
        "    datasets = datasets.drop(columns=columns_to_remove_redundant, errors='ignore')\n",
        "\n",
        "    #We checked for duplicates using the 'FictiveIDNumber' column, which uniquely identifies complaints. We noticed some complaints sharing the same 'FictiveIDNumber'.\n",
        "    #However, upon closer inspection, we found that these rows represent the same complainant reporting multiple charges in the StatisticGroup.\n",
        "    #To handle this, we can apply one-hot encoding to the StatisticGroup column, allowing each charge to be represented as a separate feature.\n",
        "\n",
        "    #Encoding the Data\n",
        "    categorical_columns = ['PoliceStation']\n",
        "    data_encoded = pd.get_dummies(datasets, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    #Removing the duplicaite rows\n",
        "    data_encoded = data_encoded.drop_duplicates()\n",
        "\n",
        "    #Feature Engineering\n",
        "    data_encoded['Quarter_numeric'] = data_encoded['Quarter'].str.extract('(\\d)').astype(int)\n",
        "    data_encoded['Quarter_sin'] = np.sin(2 * np.pi * data_encoded['Quarter_numeric'] / 4)\n",
        "    data_encoded['Quarter_cos'] = np.cos(2 * np.pi * data_encoded['Quarter_numeric'] / 4)\n",
        "    data_encoded['CrimeTrend'] = data_encoded.groupby('Year')['Year'].transform('count')\n",
        "\n",
        "    # נרמול ערכים מספריים\n",
        "    numeric_columns = ['CrimeTrend']\n",
        "    if scaler is None:\n",
        "        scaler = MinMaxScaler()\n",
        "    if fit_scaler:\n",
        "        data_encoded[numeric_columns] = scaler.fit_transform(data_encoded[numeric_columns])\n",
        "    else:\n",
        "        data_encoded[numeric_columns] = scaler.transform(data_encoded[numeric_columns])\n",
        "\n",
        "    #Now we can remove the ID column\n",
        "    data_encoded = data_encoded.drop(columns=['FictiveIDNumber','Quarter'], errors='ignore')\n",
        "\n",
        "    return data_encoded, train_mappings, scaler"
      ],
      "metadata": {
        "id": "AnWUf3rM4L9n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}