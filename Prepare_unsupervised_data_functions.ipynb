{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orifelszer/CrimeData/blob/eden-branch/Prepare_unsupervised_data_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Importing Required Libraries ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import RobustScaler, LabelEncoder, OneHotEncoder"
      ],
      "metadata": {
        "id": "BwDL4tPUEN-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Preprocessing(datasets):\n",
        "    \"\"\"\n",
        "    Unsupervised data preprocessing including missing value handling,\n",
        "    feature engineering, encoding, and normalization.\n",
        "    \"\"\"\n",
        "\n",
        "    # === Removing Invalid Rows ===\n",
        "    # Removing rows where 'StatisticGroupKod' is -1 due to a typing error in the data\n",
        "    datasets = datasets[datasets['StatisticGroupKod'] != -1]\n",
        "\n",
        "    # === Handling Missing Values ===\n",
        "    # Creating mappings for missing values based on existing data\n",
        "    train_mappings = {\n",
        "        'StatisticArea': datasets.dropna(subset=['StatisticArea'])\n",
        "                                .set_index('StatisticAreaKod')['StatisticArea'].to_dict(),\n",
        "        'Yeshuv': datasets.dropna(subset=['Yeshuv'])\n",
        "                          .set_index('YeshuvKod')['Yeshuv'].to_dict()\n",
        "    }\n",
        "\n",
        "    # Filling missing values based on mappings\n",
        "    datasets.loc[:, 'StatisticArea'] = datasets['StatisticArea'].fillna(\n",
        "        datasets['StatisticAreaKod'].map(train_mappings['StatisticArea']))\n",
        "    datasets.loc[:, 'Yeshuv'] = datasets['Yeshuv'].fillna(\n",
        "        datasets['YeshuvKod'].map(train_mappings['Yeshuv']))\n",
        "\n",
        "    # === Data Cleaning and Column Removal ===\n",
        "    # Removing rows where 'Yeshuv' is still missing\n",
        "    datasets = datasets.dropna(subset=['Yeshuv'])\n",
        "\n",
        "    # Removing columns with excessive missing values (more than 85% missing data)\n",
        "    columns_to_remove_85 = ['municipalKod', 'municipalName']\n",
        "    datasets = datasets.drop(columns=columns_to_remove_85, errors='ignore')\n",
        "\n",
        "    # Removing columns that were used for mapping and are now redundant\n",
        "    columns_to_remove_after_fill = ['StatisticAreaKod', 'YeshuvKod']\n",
        "    datasets = datasets.drop(columns=columns_to_remove_after_fill, errors='ignore')\n",
        "\n",
        "    # Removing additional redundant columns\n",
        "    columns_to_remove_redundant = [\n",
        "        'PoliceMerhavKod', 'PoliceDistrictKod', 'PoliceStationKod', 'StatisticTypeKod', 'StatisticGroupKod']\n",
        "    datasets = datasets.drop(columns=columns_to_remove_redundant, errors='ignore')\n",
        "\n",
        "    # === Handling Missing Values for 'StatisticArea' ===\n",
        "    # Probabilistic filling for 'StatisticArea' based on the most frequent value within 'Yeshuv'\n",
        "    def fill_statistic_area_random(yeshuv_group):\n",
        "        modes = yeshuv_group.mode()\n",
        "        if len(modes) > 1:\n",
        "            return np.random.choice(modes)\n",
        "        elif len(modes) == 1:  # תיקון תחביר כאן\n",
        "            return modes.iloc[0]\n",
        "        else:\n",
        "            return np.nan\n",
        "\n",
        "    # Applying the probabilistic filling method for missing 'StatisticArea' values\n",
        "    pd.set_option('future.no_silent_downcasting', True)\n",
        "    datasets.loc[:, 'StatisticArea'] = datasets.groupby('Yeshuv')['StatisticArea'].transform(\n",
        "        lambda x: x.fillna(fill_statistic_area_random(x)).infer_objects(copy=False))\n",
        "    # Dropping rows where 'StatisticArea' could not be filled\n",
        "    datasets = datasets.dropna(subset=['StatisticArea'])\n",
        "\n",
        "    # === Feature Engineering and Transformation ===\n",
        "    # Creating Cyclical Time Features for the Quarter (sin/cos transformation)\n",
        "    datasets['Quarter_numeric'] = datasets['Quarter'].str.extract(r'(\\d)').astype(int)\n",
        "    datasets['Quarter_sin'] = np.sin(2 * np.pi * datasets['Quarter_numeric'] / 4)\n",
        "    datasets['Quarter_cos'] = np.cos(2 * np.pi * datasets['Quarter_numeric'] / 4)\n",
        "\n",
        "    # Calculating Crime Rate and Annual Crime Trends based on historical data\n",
        "    datasets['YeshuvCrimeRate'] = datasets.groupby('Yeshuv')['Yeshuv'].transform('count')\n",
        "    datasets['CrimeTrend'] = datasets.groupby('Year')['Year'].transform('count')\n",
        "\n",
        "    # Adding interaction features: CrimeTrend multiplied by YeshuvCrimeRate\n",
        "    datasets['CrimeTrend_CrimeRate'] = datasets['CrimeTrend'] * datasets['YeshuvCrimeRate']\n",
        "\n",
        "    # Calculating average crime rate per police station\n",
        "    station_crime_rate = datasets.groupby('PoliceStation')['YeshuvCrimeRate'].transform('mean')\n",
        "    datasets['StationCrimeRateAvg'] = station_crime_rate\n",
        "\n",
        "    # Calculating historical crime rate per Yeshuv\n",
        "    datasets['YeshuvHistoricalCrimeRate'] = datasets.groupby('Yeshuv')['YeshuvCrimeRate'].transform('mean')\n",
        "\n",
        "    # === Urban vs. Rural Classification ===\n",
        "     # Calculating the number of nearby police stations within the same district\n",
        "    datasets['StationsNearbyCount'] = datasets.groupby('PoliceDistrict')['PoliceStation'].transform('nunique')\n",
        "\n",
        "    # Defining a list of cities in Hebrew for classification purposes\n",
        "    hebrew_cities = [\n",
        "        \"אום אל-פחם\", \"אופקים\", \"אור יהודה\", \"אור עקיבא\", \"אילת\", \"אלעד\", \"אריאל\",\n",
        "        \"אשדוד\", \"אשקלון\", \"באקה אל-גרביה\", \"באר שבע\", \"בית שאן\", \"בית שמש\", \"ביתר עילית\",\n",
        "        \"בני ברק\", \"בת ים\", \"גבעת שמואל\", \"גבעתיים\", \"דימונה\", \"הוד השרון\", \"הרצלייה\",\n",
        "        \"חדרה\", \"חולון\", \"חיפה\", \"טבריה\", \"טייבה\", \"טירה\", \"טירת כרמל\", \"טמרה\",\n",
        "        \"יבנה\", \"יהוד\", \"יקנעם עילית\", \"ירושלים\", \"כפר יונה\", \"כפר סבא\", \"כפר קאסם\",\n",
        "        \"כרמיאל\", \"לוד\", \"מגדל העמק\", \"מודיעין-מכבים-רעות\", \"מודיעין עילית\", \"מעלה אדומים\",\n",
        "        \"מעלות-תרשיחא\", \"נהרייה\", \"נס ציונה\", \"נצרת\", \"נצרת עילית\", \"נשר\", \"נתיבות\",\n",
        "        \"נתניה\", \"סח'נין\", \"עכו\", \"עפולה\", \"עראבה\", \"ערד\", \"פתח תקווה\", \"צפת\",\n",
        "        \"קלנסווה\", \"קריית אונו\", \"קריית אתא\", \"קריית ביאליק\", \"קריית גת\", \"קריית ים\",\n",
        "        \"קריית מוצקין\", \"קריית מלאכי\", \"קריית שמונה\", \"ראש העין\", \"ראשון לציון\",\n",
        "        \"רהט\", \"רחובות\", \"רמלה\", \"רמת גן\", \"רמת השרון\", \"רעננה\", \"שדרות\", \"שפרעם\",\n",
        "        \"תל אביב - יפו\"\n",
        "    ]\n",
        "\n",
        "    # Creating a new column classifying Yeshuv as 'City' or 'Moshav'\n",
        "    datasets['CityOrMoshav'] = datasets['Yeshuv'].apply(lambda x: 'City' if x in hebrew_cities else 'Moshav')\n",
        "\n",
        "    # === Encoding Categorical Variables ===\n",
        "    # Applying One-Hot Encoding to 'CityOrMoshav'\n",
        "    datasets = pd.get_dummies(datasets, columns=['CityOrMoshav'], drop_first=True)\n",
        "\n",
        "    # Applying Label Encoding to categorical columns\n",
        "    categorical_columns = ['Yeshuv', 'PoliceStation', 'StatisticArea', 'PoliceMerhav', 'PoliceDistrict', 'StatisticType', 'FictiveIDNumber', 'StatisticGroup']\n",
        "    label_encoders = {}\n",
        "    for col in categorical_columns:\n",
        "        le = LabelEncoder()\n",
        "        datasets[col] = le.fit_transform(datasets[col])\n",
        "        label_encoders[col] = le\n",
        "\n",
        "    # === Normalization of Numeric Features ===\n",
        "    numeric_columns = ['YeshuvCrimeRate', 'CrimeTrend', 'CrimeTrend_CrimeRate', 'StationCrimeRateAvg',\n",
        "                       'YeshuvHistoricalCrimeRate', 'StationsNearbyCount']\n",
        "\n",
        "    scaler = RobustScaler()\n",
        "    datasets[numeric_columns] = scaler.fit_transform(datasets[numeric_columns])\n",
        "\n",
        "    # Preventing negative values by shifting the data\n",
        "    for col in numeric_columns:\n",
        "        min_value = datasets[col].min()\n",
        "        if min_value < 0:\n",
        "            datasets[col] = datasets[col] + abs(min_value) + 1e-5\n",
        "\n",
        "    # === Data Cleaning and Deduplication ===\n",
        "    datasets = datasets.drop_duplicates()\n",
        "    datasets = datasets.drop(columns=['Quarter'], errors='ignore')\n",
        "\n",
        "    # Reducing memory usage\n",
        "    def optimize_data_types(df):\n",
        "        for col in df.columns:\n",
        "            col_type = df[col].dtype\n",
        "            if col_type == 'object':\n",
        "                df[col] = df[col].astype('category')\n",
        "            elif col_type == 'float64':\n",
        "                df[col] = df[col].astype('float32')\n",
        "            elif col_type == 'int64':\n",
        "                df[col] = df[col].astype('int32')\n",
        "        return df\n",
        "\n",
        "    datasets = optimize_data_types(datasets)\n",
        "\n",
        "    return datasets, label_encoders"
      ],
      "metadata": {
        "id": "LenzHh7S03ku"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}