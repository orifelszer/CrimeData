{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO62VWWqbSUK7jbCOGiAkDm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orifelszer/CrimeData/blob/eden-branch/Prepare_supervised_data_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "fcloJ-pTnuGX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Mapping and Imputation for Missing Values ===\n",
        "class FillMissingValues(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        # Creating mappings for 'StatisticArea' and 'Yeshuv' to fill missing values\n",
        "        if self.train_mappings is None:\n",
        "            self.train_mappings = {\n",
        "                'StatisticArea': X.dropna(subset=['StatisticArea'])\n",
        "                                   .set_index('StatisticAreaKod')['StatisticArea'].to_dict(),\n",
        "                'Yeshuv': X.dropna(subset=['Yeshuv'])\n",
        "                           .set_index('YeshuvKod')['Yeshuv'].to_dict()\n",
        "            }\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Fill missing values using the mappings created above\n",
        "        X_filled = X.copy()\n",
        "        X_filled['StatisticArea'] = X_filled['StatisticArea'].fillna(\n",
        "            X_filled['StatisticAreaKod'].map(self.train_mappings['StatisticArea'])\n",
        "        )\n",
        "        X_filled['Yeshuv'] = X_filled['Yeshuv'].fillna(\n",
        "            X_filled['YeshuvKod'].map(self.train_mappings['Yeshuv'])\n",
        "        )\n",
        "        return X_filled\n",
        "\n",
        "# === Data Cleaning and Column Removal ===\n",
        "class DataCleaning(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Remove rows where 'Yeshuv' is still missing\n",
        "        X_cleaned = X.copy()\n",
        "        X_cleaned = X_cleaned.dropna(subset=['Yeshuv'])\n",
        "\n",
        "        # Remove columns with excessive missing values (more than 85% missing)\n",
        "        columns_to_remove_85 = ['municipalKod', 'municipalName']\n",
        "        X_cleaned = X_cleaned.drop(columns=columns_to_remove_85, errors='ignore')\n",
        "\n",
        "        # Remove columns that may cause data leakage\n",
        "        X_cleaned = X_cleaned.drop(columns=['StatisticTypeKod', 'StatisticType'], errors='ignore')\n",
        "\n",
        "        # Remove columns used for mapping that are now redundant\n",
        "        columns_to_remove_after_fill = ['StatisticAreaKod', 'YeshuvKod']\n",
        "        X_cleaned = X_cleaned.drop(columns=columns_to_remove_after_fill, errors='ignore')\n",
        "\n",
        "        # Remove additional redundant columns\n",
        "        columns_to_remove_redundant = ['PoliceMerhavKod', 'PoliceDistrictKod', 'PoliceStationKod']\n",
        "        X_cleaned = X_cleaned.drop(columns=columns_to_remove_redundant, errors='ignore')\n",
        "\n",
        "        return X_cleaned\n",
        "\n",
        "# === Handling Missing Values for 'StatisticArea' ===\n",
        "class ImputeStatisticArea(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_filled = X.copy()\n",
        "        # Create a binary indicator for missing 'StatisticArea' values\n",
        "        X_filled['is_missing_StatisticArea'] = X_filled['StatisticArea'].isnull().astype(int)\n",
        "\n",
        "        # Define a helper function for probabilistic filling\n",
        "        def fill_statistic_area_random(yeshuv_group):\n",
        "            modes = yeshuv_group.mode()\n",
        "            if len(modes) > 1:\n",
        "                return np.random.choice(modes)\n",
        "            elif len(modes) == 1:\n",
        "                return modes.iloc[0]\n",
        "            else:\n",
        "                return np.nan\n",
        "\n",
        "        # Apply probabilistic filling for missing 'StatisticArea'\n",
        "        X_filled['StatisticArea'] = X_filled.groupby('Yeshuv')['StatisticArea'].transform(\n",
        "            lambda x: x.fillna(fill_statistic_area_random(x)))\n",
        "\n",
        "        # Drop rows where 'StatisticArea' could not be filled\n",
        "        X_filled = X_filled.dropna(subset=['StatisticArea'])\n",
        "\n",
        "        return X_filled\n",
        "\n",
        "# === Feature Engineering and Transformation ===\n",
        "class FeatureEngineering(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_fe = X.copy()\n",
        "\n",
        "        # Creating Cyclical Time Features for the Quarter (sin/cos transformation)\n",
        "        X_fe['Quarter_numeric'] = X_fe['Quarter'].str.extract(r'(\\d)').astype(int)\n",
        "        X_fe['Quarter_sin'] = np.sin(2 * np.pi * X_fe['Quarter_numeric'] / 4)\n",
        "        X_fe['Quarter_cos'] = np.cos(2 * np.pi * X_fe['Quarter_numeric'] / 4)\n",
        "\n",
        "        # Calculating Crime Rates and Annual Trends\n",
        "        X_fe['YeshuvCrimeRate'] = X_fe.groupby('Yeshuv')['Yeshuv'].transform('count')\n",
        "        X_fe['CrimeTrend'] = X_fe.groupby('Year')['Year'].transform('count')\n",
        "\n",
        "        # Adding Interaction Features\n",
        "        X_fe['CrimeTrend_CrimeRate'] = X_fe['CrimeTrend'] * X_fe['YeshuvCrimeRate']\n",
        "\n",
        "        # Calculating Average Crime Rates\n",
        "        X_fe['StationCrimeRateAvg'] = X_fe.groupby('PoliceStation')['YeshuvCrimeRate'].transform('mean')\n",
        "        X_fe['YeshuvHistoricalCrimeRate'] = X_fe.groupby('Yeshuv')['YeshuvCrimeRate'].transform('mean')\n",
        "\n",
        "        # Counting Nearby Police Stations\n",
        "        X_fe['StationsNearbyCount'] = X_fe.groupby('PoliceDistrict')['PoliceStation'].transform('nunique')\n",
        "\n",
        "        return X_fe\n",
        "\n",
        "# === Urban vs. Rural Classification ===\n",
        "class UrbanRuralClassification(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_classified = X.copy()\n",
        "\n",
        "        # Defining the list of Hebrew cities\n",
        "        hebrew_cities = [\n",
        "        \"אום אל-פחם\", \"אופקים\", \"אור יהודה\", \"אור עקיבא\", \"אילת\", \"אלעד\", \"אריאל\",\n",
        "        \"אשדוד\", \"אשקלון\", \"באקה אל-גרביה\", \"באר שבע\", \"בית שאן\", \"בית שמש\", \"ביתר עילית\",\n",
        "        \"בני ברק\", \"בת ים\", \"גבעת שמואל\", \"גבעתיים\", \"דימונה\", \"הוד השרון\", \"הרצלייה\",\n",
        "        \"חדרה\", \"חולון\", \"חיפה\", \"טבריה\", \"טייבה\", \"טירה\", \"טירת כרמל\", \"טמרה\",\n",
        "        \"יבנה\", \"יהוד\", \"יקנעם עילית\", \"ירושלים\", \"כפר יונה\", \"כפר סבא\", \"כפר קאסם\",\n",
        "        \"כרמיאל\", \"לוד\", \"מגדל העמק\", \"מודיעין-מכבים-רעות\", \"מודיעין עילית\", \"מעלה אדומים\",\n",
        "        \"מעלות-תרשיחא\", \"נהרייה\", \"נס ציונה\", \"נצרת\", \"נצרת עילית\", \"נשר\", \"נתיבות\",\n",
        "        \"נתניה\", \"סח'נין\", \"עכו\", \"עפולה\", \"עראבה\", \"ערד\", \"פתח תקווה\", \"צפת\",\n",
        "        \"קלנסווה\", \"קריית אונו\", \"קריית אתא\", \"קריית ביאליק\", \"קריית גת\", \"קריית ים\",\n",
        "        \"קריית מוצקין\", \"קריית מלאכי\", \"קריית שמונה\", \"ראש העין\", \"ראשון לציון\",\n",
        "        \"רהט\", \"רחובות\", \"רמלה\", \"רמת גן\", \"רמת השרון\", \"רעננה\", \"שדרות\", \"שפרעם\",\n",
        "        \"תל אביב - יפו\"\n",
        "        ]\n",
        "\n",
        "        # Creating a new column classifying Yeshuv as 'City' or 'Moshav'\n",
        "        X_classified['CityOrMoshav'] = X_classified['Yeshuv'].apply(\n",
        "            lambda x: 'City' if x in hebrew_cities else 'Moshav')\n",
        "\n",
        "        return X_classified\n",
        "\n",
        "# === Encoding Categorical Variables and Normalizing Numeric Features ===\n",
        "class EncodingAndScaling(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, fit_scaler=True):\n",
        "        self.label_encoders = {}\n",
        "        self.scaler = RobustScaler()\n",
        "        self.fit_scaler = fit_scaler\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Applying Label Encoding for categorical columns\n",
        "        self.categorical_columns = ['Yeshuv', 'PoliceStation', 'StatisticArea', 'PoliceMerhav', 'PoliceDistrict']\n",
        "        self.numeric_columns = ['YeshuvCrimeRate', 'CrimeTrend', 'CrimeTrend_CrimeRate',\n",
        "                                'StationCrimeRateAvg', 'YeshuvHistoricalCrimeRate', 'StationsNearbyCount']\n",
        "\n",
        "        for col in self.categorical_columns:\n",
        "            le = LabelEncoder()\n",
        "            le.fit(X[col])\n",
        "            self.label_encoders[col] = le\n",
        "\n",
        "        # Fitting the scaler only if required\n",
        "        if self.fit_scaler:\n",
        "            self.scaler.fit(X[self.numeric_columns])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_transformed = X.copy()\n",
        "\n",
        "        # === Applying One-Hot Encoding to 'CityOrMoshav' ===\n",
        "        X_transformed = pd.get_dummies(X_transformed, columns=['CityOrMoshav'], drop_first=True)\n",
        "\n",
        "        # === Applying Label Encoding to categorical columns ===\n",
        "        for col, le in self.label_encoders.items():\n",
        "            X_transformed[col] = le.transform(X_transformed[col])\n",
        "\n",
        "        # === Normalization of Numeric Features ===\n",
        "        if self.fit_scaler:\n",
        "            X_transformed[self.numeric_columns] = self.scaler.transform(X_transformed[self.numeric_columns])\n",
        "\n",
        "        # Preventing negative values by adding a small positive constant\n",
        "        for col in self.numeric_columns:\n",
        "            min_value = X_transformed[col].min()\n",
        "            if min_value < 0:\n",
        "                X_transformed[col] = X_transformed[col] + abs(min_value) + 1e-5\n",
        "\n",
        "        return X_transformed\n",
        "\n",
        "# === Memory Usage Reduction ===\n",
        "class MemoryReduction(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_optimized = X.copy()\n",
        "\n",
        "        # Optimize data types to reduce memory usage\n",
        "        for col in X_optimized.columns:\n",
        "            col_type = X_optimized[col].dtype\n",
        "            if col_type == 'object':\n",
        "                X_optimized[col] = X_optimized[col].astype('category')\n",
        "            elif col_type == 'float64':\n",
        "                X_optimized[col] = X_optimized[col].astype('float32')\n",
        "            elif col_type == 'int64':\n",
        "                X_optimized[col] = X_optimized[col].astype('int32')\n",
        "\n",
        "        return X_optimized\n",
        "\n",
        "# === Data Cleaning and Deduplication ===\n",
        "class DataCleaningAndDeduplication(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_cleaned = X.copy()\n",
        "\n",
        "        # Removing duplicate rows\n",
        "        X_cleaned = X_cleaned.drop_duplicates()\n",
        "\n",
        "        # Removing specific columns that are no longer needed\n",
        "        X_cleaned = X_cleaned.drop(columns=['FictiveIDNumber', 'Quarter'], errors='ignore')\n",
        "\n",
        "        return X_cleaned\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# === Creating the full preprocessing pipeline ===\n",
        "pipeline = Pipeline([\n",
        "    ('data_cleaning', DataCleaningAndDeduplication()),\n",
        "    ('fill_missing', FillMissingValues()),\n",
        "    ('feature_engineering', FeatureEngineering()),\n",
        "    ('urban_rural_classification', UrbanRuralClassification()),\n",
        "    ('encoding_scaling', EncodingAndScaling()),\n",
        "    ('memory_reduction', MemoryReduction())\n",
        "])"
      ],
      "metadata": {
        "id": "3OoSV9H7cFFt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # === Importing Required Libraries ===\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.preprocessing import RobustScaler, LabelEncoder, OneHotEncoder"
      ],
      "metadata": {
        "id": "BwDL4tPUEN-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def Preprocessing(datasets, train_mappings=None, scaler=None, fit_scaler=False):\n",
        "#     \"\"\"\n",
        "#     Function for preprocessing datasets including missing value handling,\n",
        "#     feature engineering, and label encoding.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # === Mapping and Imputation for Missing Values ===\n",
        "#     # Creating mappings for 'StatisticArea' and 'Yeshuv' to fill missing values\n",
        "#     if train_mappings is None:\n",
        "#         train_mappings = {\n",
        "#             'StatisticArea': datasets.dropna(subset=['StatisticArea'])\n",
        "#                                     .set_index('StatisticAreaKod')['StatisticArea'].to_dict(),\n",
        "#             'Yeshuv': datasets.dropna(subset=['Yeshuv'])\n",
        "#                               .set_index('YeshuvKod')['Yeshuv'].to_dict()\n",
        "#         }\n",
        "\n",
        "#     # Fill missing values using the mappings created above\n",
        "#     datasets['StatisticArea'] = datasets['StatisticArea'].fillna(\n",
        "#         datasets['StatisticAreaKod'].map(train_mappings['StatisticArea']))\n",
        "#     datasets['Yeshuv'] = datasets['Yeshuv'].fillna(\n",
        "#         datasets['YeshuvKod'].map(train_mappings['Yeshuv']))\n",
        "\n",
        "#     # === Data Cleaning and Column Removal ===\n",
        "#     # Remove rows where 'Yeshuv' is still missing\n",
        "#     datasets = datasets.dropna(subset=['Yeshuv'])\n",
        "\n",
        "#     # Remove columns with excessive missing values (more than 85% missing)\n",
        "#     columns_to_remove_85 = ['municipalKod', 'municipalName']\n",
        "#     datasets = datasets.drop(columns=columns_to_remove_85, errors='ignore')\n",
        "\n",
        "#     # Remove columns that may cause data leakage as they are subcategories of the target variable\n",
        "#     datasets = datasets.drop(columns=['StatisticTypeKod', 'StatisticType'], errors='ignore')\n",
        "\n",
        "#     # Remove columns that duplicate information already stored in text format\n",
        "#     # These columns ('StatisticAreaKod' and 'YeshuvKod') were previously used for mapping and are now redundant\n",
        "#     columns_to_remove_after_fill = ['StatisticAreaKod', 'YeshuvKod']\n",
        "#     datasets = datasets.drop(columns=columns_to_remove_after_fill, errors='ignore')\n",
        "\n",
        "#     # Remove additional redundant columns related to police districts and areas\n",
        "#     columns_to_remove_redundant = [\n",
        "#         'PoliceMerhavKod', 'PoliceDistrictKod', 'PoliceStationKod']\n",
        "#     datasets = datasets.drop(columns=columns_to_remove_redundant, errors='ignore')\n",
        "\n",
        "#     # === Handling Missing Values for 'StatisticArea' ===\n",
        "#     # Create a binary indicator for missing 'StatisticArea' values\n",
        "#     datasets['is_missing_StatisticArea'] = datasets['StatisticArea'].isnull().astype(int)\n",
        "\n",
        "#     # Probabilistic filling for 'StatisticArea' based on the most frequent value within 'Yeshuv'\n",
        "#     def fill_statistic_area_random(yeshuv_group):\n",
        "#         modes = yeshuv_group.mode()\n",
        "#         if len(modes) > 1:\n",
        "#             return np.random.choice(modes)\n",
        "#         elif len(modes) == 1:  # תיקון תחביר כאן\n",
        "#             return modes.iloc[0]\n",
        "#         else:\n",
        "#             return np.nan\n",
        "\n",
        "#     # Apply probabilistic filling for missing 'StatisticArea'\n",
        "#     datasets['StatisticArea'] = datasets.groupby('Yeshuv')['StatisticArea'].transform(\n",
        "#         lambda x: x.fillna(fill_statistic_area_random(x)))\n",
        "#     datasets = datasets.dropna(subset=['StatisticArea'])\n",
        "\n",
        "#     # === Feature Engineering and Transformation ===\n",
        "#     # Creating Cyclical Time Features for the Quarter (sin/cos transformation)\n",
        "#     datasets['Quarter_numeric'] = datasets['Quarter'].str.extract(r'(\\d)').astype(int)\n",
        "#     datasets['Quarter_sin'] = np.sin(2 * np.pi * datasets['Quarter_numeric'] / 4)\n",
        "#     datasets['Quarter_cos'] = np.cos(2 * np.pi * datasets['Quarter_numeric'] / 4)\n",
        "\n",
        "#     # Calculating Crime Rate and Annual Crime Trends based on historical data\n",
        "#     datasets['YeshuvCrimeRate'] = datasets.groupby('Yeshuv')['Yeshuv'].transform('count')\n",
        "#     datasets['CrimeTrend'] = datasets.groupby('Year')['Year'].transform('count')\n",
        "\n",
        "#     # Adding interaction features: CrimeTrend multiplied by YeshuvCrimeRate\n",
        "#     datasets['CrimeTrend_CrimeRate'] = datasets['CrimeTrend'] * datasets['YeshuvCrimeRate']\n",
        "\n",
        "#     # Calculating average crime rate per police station\n",
        "#     station_crime_rate = datasets.groupby('PoliceStation')['YeshuvCrimeRate'].transform('mean')\n",
        "#     datasets['StationCrimeRateAvg'] = station_crime_rate\n",
        "\n",
        "#     # Calculating historical crime rate per Yeshuv\n",
        "#     datasets['YeshuvHistoricalCrimeRate'] = datasets.groupby('Yeshuv')['YeshuvCrimeRate'].transform('mean')\n",
        "\n",
        "#     # Calculating the number of nearby police stations within the same district\n",
        "#     datasets['StationsNearbyCount'] = datasets.groupby('PoliceDistrict')['PoliceStation'].transform('nunique')\n",
        "\n",
        "#     # === Urban vs. Rural Classification ===\n",
        "#     # Defining a list of cities in Hebrew for classification purposes\n",
        "#     hebrew_cities = [\n",
        "#         \"אום אל-פחם\", \"אופקים\", \"אור יהודה\", \"אור עקיבא\", \"אילת\", \"אלעד\", \"אריאל\",\n",
        "#         \"אשדוד\", \"אשקלון\", \"באקה אל-גרביה\", \"באר שבע\", \"בית שאן\", \"בית שמש\", \"ביתר עילית\",\n",
        "#         \"בני ברק\", \"בת ים\", \"גבעת שמואל\", \"גבעתיים\", \"דימונה\", \"הוד השרון\", \"הרצלייה\",\n",
        "#         \"חדרה\", \"חולון\", \"חיפה\", \"טבריה\", \"טייבה\", \"טירה\", \"טירת כרמל\", \"טמרה\",\n",
        "#         \"יבנה\", \"יהוד\", \"יקנעם עילית\", \"ירושלים\", \"כפר יונה\", \"כפר סבא\", \"כפר קאסם\",\n",
        "#         \"כרמיאל\", \"לוד\", \"מגדל העמק\", \"מודיעין-מכבים-רעות\", \"מודיעין עילית\", \"מעלה אדומים\",\n",
        "#         \"מעלות-תרשיחא\", \"נהרייה\", \"נס ציונה\", \"נצרת\", \"נצרת עילית\", \"נשר\", \"נתיבות\",\n",
        "#         \"נתניה\", \"סח'נין\", \"עכו\", \"עפולה\", \"עראבה\", \"ערד\", \"פתח תקווה\", \"צפת\",\n",
        "#         \"קלנסווה\", \"קריית אונו\", \"קריית אתא\", \"קריית ביאליק\", \"קריית גת\", \"קריית ים\",\n",
        "#         \"קריית מוצקין\", \"קריית מלאכי\", \"קריית שמונה\", \"ראש העין\", \"ראשון לציון\",\n",
        "#         \"רהט\", \"רחובות\", \"רמלה\", \"רמת גן\", \"רמת השרון\", \"רעננה\", \"שדרות\", \"שפרעם\",\n",
        "#         \"תל אביב - יפו\"\n",
        "#     ]\n",
        "\n",
        "#     # Creating a new column classifying Yeshuv as 'City' or 'Moshav'\n",
        "#     datasets['CityOrMoshav'] = datasets['Yeshuv'].apply(lambda x: 'City' if x in hebrew_cities else 'Moshav')\n",
        "\n",
        "#     # === Encoding Categorical Variables ===\n",
        "#     # Applying One-Hot Encoding to 'CityOrMoshav'\n",
        "#     datasets = pd.get_dummies(datasets, columns=['CityOrMoshav'], drop_first=True)\n",
        "\n",
        "#     # Applying Label Encoding to categorical columns\n",
        "#     categorical_columns = ['Yeshuv', 'PoliceStation', 'StatisticArea', 'PoliceMerhav', 'PoliceDistrict']\n",
        "#     label_encoders = {}\n",
        "#     for col in categorical_columns:\n",
        "#         le = LabelEncoder()\n",
        "#         datasets[col] = le.fit_transform(datasets[col])\n",
        "#         label_encoders[col] = le\n",
        "\n",
        "#     # === Normalization of Numeric Features ===\n",
        "#     numeric_columns = ['YeshuvCrimeRate', 'CrimeTrend', 'CrimeTrend_CrimeRate', 'StationCrimeRateAvg',\n",
        "#                        'YeshuvHistoricalCrimeRate', 'StationsNearbyCount']\n",
        "\n",
        "#     if scaler is None:\n",
        "#         scaler = RobustScaler()\n",
        "#     if fit_scaler:\n",
        "#         datasets[numeric_columns] = scaler.fit_transform(datasets[numeric_columns])\n",
        "#     else:\n",
        "#         datasets[numeric_columns] = scaler.transform(datasets[numeric_columns])\n",
        "\n",
        "#     # Prevent negative values by adding a small positive constant\n",
        "#     for col in numeric_columns:\n",
        "#         min_value = datasets[col].min()\n",
        "#         if min_value < 0:\n",
        "#             datasets[col] = datasets[col] + abs(min_value) + 1e-5\n",
        "\n",
        "#     # === Data Cleaning and Deduplication ===\n",
        "#     datasets = datasets.drop_duplicates()\n",
        "#     datasets = datasets.drop(columns=['FictiveIDNumber', 'Quarter'], errors='ignore')\n",
        "\n",
        "#         # === Memory Usage Reduction ===\n",
        "#     def optimize_data_types(df):\n",
        "#         for col in df.columns:\n",
        "#             col_type = df[col].dtype\n",
        "#             if col_type == 'object':\n",
        "#                 df[col] = df[col].astype('category')\n",
        "#             elif col_type == 'float64':\n",
        "#                 df[col] = df[col].astype('float32')\n",
        "#             elif col_type == 'int64':\n",
        "#                 df[col] = df[col].astype('int32')\n",
        "#         return df\n",
        "\n",
        "#     datasets = optimize_data_types(datasets)\n",
        "\n",
        "#     return datasets, train_mappings, scaler, label_encoders"
      ],
      "metadata": {
        "id": "LenzHh7S03ku"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}