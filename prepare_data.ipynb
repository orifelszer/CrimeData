{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMwsKqNaW9y6esXHyJ5Nk9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orifelszer/CrimeData/blob/eden-branch/prepare_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing necessary libraries for data preprocessing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import RobustScaler, LabelEncoder"
      ],
      "metadata": {
        "id": "BwDL4tPUEN-m"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Preprocessing(datasets, train_mappings=None, scaler=None, fit_scaler=False):\n",
        "    \"\"\"\n",
        "    Function for preprocessing datasets including missing value handling,\n",
        "    feature engineering, and label encoding.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate mappings for 'StatisticArea' and 'Yeshuv' if not provided\n",
        "    if train_mappings is None:\n",
        "        train_mappings = {\n",
        "            'StatisticArea': datasets.dropna(subset=['StatisticArea'])\n",
        "                                    .set_index('StatisticAreaKod')['StatisticArea'].to_dict(),\n",
        "            'Yeshuv': datasets.dropna(subset=['Yeshuv'])\n",
        "                              .set_index('YeshuvKod')['Yeshuv'].to_dict()\n",
        "        }\n",
        "\n",
        "    # Fill missing values using the mappings\n",
        "    datasets['StatisticArea'] = datasets['StatisticArea'].fillna(\n",
        "        datasets['StatisticAreaKod'].map(train_mappings['StatisticArea']))\n",
        "    datasets['Yeshuv'] = datasets['Yeshuv'].fillna(\n",
        "        datasets['YeshuvKod'].map(train_mappings['Yeshuv']))\n",
        "\n",
        "    # Remove rows where 'Yeshuv' is still missing\n",
        "    datasets = datasets.dropna(subset=['Yeshuv'])\n",
        "\n",
        "    # Drop columns with over 85% missing values\n",
        "    columns_to_remove_85 = ['municipalKod', 'municipalName']\n",
        "    datasets = datasets.drop(columns=columns_to_remove_85, errors='ignore')\n",
        "\n",
        "    # Drop redundant columns that do not contribute to the analysis\n",
        "    datasets = datasets.drop(columns=['StatisticTypeKod', 'StatisticType'], errors='ignore')\n",
        "\n",
        "    # Remove additional columns no longer useful after filling missing values\n",
        "    columns_to_remove_after_fill = ['StatisticAreaKod', 'YeshuvKod']\n",
        "    datasets = datasets.drop(columns=columns_to_remove_after_fill, errors='ignore')\n",
        "\n",
        "    # Create a binary indicator for missing 'StatisticArea' values\n",
        "    datasets['is_missing_StatisticArea'] = datasets['StatisticArea'].isnull().astype(int)\n",
        "\n",
        "    # Fill remaining missing 'StatisticArea' using the most common value within 'PoliceDistrict'\n",
        "    statistic_area_map = datasets.groupby('PoliceDistrict')['StatisticArea'].agg(\n",
        "        lambda x: x.mode().iloc[0] if not x.mode().empty else None).to_dict()\n",
        "    datasets['StatisticArea'] = datasets['StatisticArea'].fillna(\n",
        "        datasets['PoliceDistrict'].map(statistic_area_map))\n",
        "\n",
        "    # Remove additional redundant columns related to police districts and areas\n",
        "    columns_to_remove_redundant = [\n",
        "        'PoliceMerhavKod', 'PoliceDistrictKod', 'PoliceStationKod',\n",
        "        'PoliceMerhav', 'PoliceDistrict'\n",
        "        ]\n",
        "    datasets = datasets.drop(columns=columns_to_remove_redundant, errors='ignore')\n",
        "\n",
        "    # Convert categorical columns to numeric using Label Encoding\n",
        "    categorical_columns = ['Yeshuv', 'PoliceStation', 'StatisticArea']\n",
        "    label_encoders = {}\n",
        "    for col in categorical_columns:\n",
        "        le = LabelEncoder()\n",
        "        datasets[col] = le.fit_transform(datasets[col])\n",
        "        label_encoders[col] = le\n",
        "\n",
        "    # Remove duplicate rows after encoding\n",
        "    datasets = datasets.drop_duplicates()\n",
        "\n",
        "    # Feature Engineering: Extract numeric quarter and create cyclical features (sin/cos)\n",
        "    datasets['Quarter_numeric'] = datasets['Quarter'].str.extract(r'(\\d)').astype(int)\n",
        "    datasets['Quarter_sin'] = np.sin(2 * np.pi * datasets['Quarter_numeric'] / 4)\n",
        "    datasets['Quarter_cos'] = np.cos(2 * np.pi * datasets['Quarter_numeric'] / 4)\n",
        "\n",
        "    # Create features for crime rate per Yeshuv and annual crime trend\n",
        "    datasets['YeshuvCrimeRate'] = datasets.groupby('Yeshuv')['Yeshuv'].transform('count')\n",
        "    datasets['CrimeTrend'] = datasets.groupby('Year')['Year'].transform('count')\n",
        "\n",
        "    # Normalize numeric columns using MinMaxScaler\n",
        "    numeric_columns = ['YeshuvCrimeRate', 'CrimeTrend']\n",
        "    # יצירת סקיילר במידת הצורך\n",
        "    if scaler is None:\n",
        "        scaler = RobustScaler()\n",
        "    # אימון הסקיילר על נתוני האימון בלבד\n",
        "    if fit_scaler:\n",
        "        datasets[numeric_columns] = scaler.fit_transform(datasets[numeric_columns])\n",
        "    else:\n",
        "        # החלת הסקיילר על נתוני הבדיקה\n",
        "        datasets[numeric_columns] = scaler.transform(datasets[numeric_columns])\n",
        "\n",
        "    # מניעת ערכים שליליים בעזרת הוספת קבוע קטן\n",
        "    for col in numeric_columns:\n",
        "        min_value = datasets[col].min()\n",
        "        if min_value < 0:\n",
        "            datasets[col] = datasets[col] + abs(min_value) + 1e-5\n",
        "\n",
        "    # Remove the ID column and the original quarter column\n",
        "    datasets = datasets.drop(columns=['FictiveIDNumber', 'Quarter'], errors='ignore')\n",
        "\n",
        "    # Return the preprocessed dataset along with the mappings and scaler for later use\n",
        "    return datasets, train_mappings, scaler, label_encoders\n"
      ],
      "metadata": {
        "id": "LenzHh7S03ku"
      },
      "execution_count": 61,
      "outputs": []
    }
  ]
}